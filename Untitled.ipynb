{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import gensim\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, GRU\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.initializers import Constant\n",
    "import numpy as np\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn import model_selection\n",
    "import tensorflow as tf\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = list(string.punctuation)\n",
    "stop = stopwords.words('english')\n",
    "dummy_stop = []\n",
    "for word in stop:\n",
    "    dummy = word[0].upper()+word[1:]\n",
    "    dummy_stop.append(dummy)\n",
    "stop = stop+dummy_stop\n",
    "# print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = []\n",
    "rating = []\n",
    "rating_checklist_2 = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
    "with open('all_reviews.tsv','r') as fp:\n",
    "    lines=fp.readlines()\n",
    "\n",
    "# count=0\n",
    "for line in lines:\n",
    "#     if count > 100:\n",
    "#         break\n",
    "    words=word_tokenize(line)\n",
    "    for i in range(len(words)):\n",
    "        dummy=\"\"\n",
    "        for j in range(len(words[i])):\n",
    "            if not words[i][j] in punctuations:\n",
    "                dummy += words[i][j]\n",
    "        words[i] = dummy\n",
    "    clean_words=[w for w in words if not w in stop]\n",
    "    if clean_words[len(clean_words)-1] in rating_checklist_2:\n",
    "        rating.append(clean_words[len(clean_words)-1])\n",
    "        clean_sym_words = [w for w in clean_words if w.isalpha()]\n",
    "        reviews.append(clean_sym_words[0:len(clean_sym_words)-1])\n",
    "    \n",
    "#     count += 1\n",
    "\n",
    "# print(reviews[0:5])\n",
    "# print(rating[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Google pre-trained word2vec embeddings\n",
    "if not os.path.exists('embedding_word2vec.txt'):\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "    filename = \"./embedding_word2vec.txt\"\n",
    "    model.save_word2vec_format(filename, binary = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.most_similar('horrible')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = {}\n",
    "# fp = open(\"embedding_word2vec.txt\")\n",
    "# for line in fp:\n",
    "#     val = line.split()\n",
    "#     word = val[0]\n",
    "#     vector_coefs = np.asarray(val[1:])\n",
    "#     embeddings[word]=vector_coefs\n",
    "# print(embeddings)\n",
    "# fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_obj = Tokenizer()\n",
    "tokenized_obj.fit_on_texts(reviews)\n",
    "sequences = tokenized_obj.texts_to_sequences(reviews)\n",
    "# print(sequences)\n",
    "word_index = tokenized_obj.word_index\n",
    "#         breakobj.word_index\n",
    "# print(word_index)\n",
    "# print(len(word_index))\n",
    "# print(len(word_index))\n",
    "maxLength = 0\n",
    "for review in sequences:\n",
    "    if maxLength < len(review):\n",
    "        maxLength = len(review)\n",
    "# print(maxLength)\n",
    "# print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = {}\n",
    "fp = open(\"embedding_word2vec.txt\")\n",
    "for line in fp:\n",
    "    val = line.split()\n",
    "    word = val[0]\n",
    "    vector_coefs = np.asarray(val[1:])\n",
    "    if word in word_index.keys():\n",
    "        embeddings[word]=vector_coefs\n",
    "# print(embeddings)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_pad = pad_sequences(sequences, maxlen = maxLength)\n",
    "rating = np.asarray(rating)\n",
    "# print(rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EmbeddingDim = 300\n",
    "num_words = len(word_index)+1\n",
    "embedding_matrix  = np.zeros((num_words, EmbeddingDim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "# print(num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(rating))\n",
    "rating_checklist = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
    "rating_matrix = np.zeros((len(rating), 11))\n",
    "for i in range(len(rating)):\n",
    "    if rating[i] in rating_checklist:\n",
    "        rating_matrix[i][int(rating[i])] = 1\n",
    "# print(rating_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model = Sequential()\n",
    "embedding_layer = Embedding(num_words, EmbeddingDim, embeddings_initializer = Constant(embedding_matrix), input_length = maxLength, trainable = False)\n",
    "train_model.add(embedding_layer)\n",
    "train_model.add(GRU(units = 100))\n",
    "train_model.add(Dense(11,activation = 'sigmoid'))\n",
    "train_model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(review_pad, rating_matrix)\n",
    "# print(X_train.shape)\n",
    "# print(Y_train.shape)\n",
    "train_model.fit(X_train, Y_train, epochs = 5, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = train_model.evaluate(X_test, Y_test)\n",
    "print(accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

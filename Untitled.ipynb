{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/coding-\n",
      "[nltk_data]     club/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/coding-\n",
      "[nltk_data]     club/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import gensim\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, GRU\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.initializers import Constant\n",
    "import numpy as np\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'I', 'Me', 'My', 'Myself', 'We', 'Our', 'Ours', 'Ourselves', 'You', \"You're\", \"You've\", \"You'll\", \"You'd\", 'Your', 'Yours', 'Yourself', 'Yourselves', 'He', 'Him', 'His', 'Himself', 'She', \"She's\", 'Her', 'Hers', 'Herself', 'It', \"It's\", 'Its', 'Itself', 'They', 'Them', 'Their', 'Theirs', 'Themselves', 'What', 'Which', 'Who', 'Whom', 'This', 'That', \"That'll\", 'These', 'Those', 'Am', 'Is', 'Are', 'Was', 'Were', 'Be', 'Been', 'Being', 'Have', 'Has', 'Had', 'Having', 'Do', 'Does', 'Did', 'Doing', 'A', 'An', 'The', 'And', 'But', 'If', 'Or', 'Because', 'As', 'Until', 'While', 'Of', 'At', 'By', 'For', 'With', 'About', 'Against', 'Between', 'Into', 'Through', 'During', 'Before', 'After', 'Above', 'Below', 'To', 'From', 'Up', 'Down', 'In', 'Out', 'On', 'Off', 'Over', 'Under', 'Again', 'Further', 'Then', 'Once', 'Here', 'There', 'When', 'Where', 'Why', 'How', 'All', 'Any', 'Both', 'Each', 'Few', 'More', 'Most', 'Other', 'Some', 'Such', 'No', 'Nor', 'Not', 'Only', 'Own', 'Same', 'So', 'Than', 'Too', 'Very', 'S', 'T', 'Can', 'Will', 'Just', 'Don', \"Don't\", 'Should', \"Should've\", 'Now', 'D', 'Ll', 'M', 'O', 'Re', 'Ve', 'Y', 'Ain', 'Aren', \"Aren't\", 'Couldn', \"Couldn't\", 'Didn', \"Didn't\", 'Doesn', \"Doesn't\", 'Hadn', \"Hadn't\", 'Hasn', \"Hasn't\", 'Haven', \"Haven't\", 'Isn', \"Isn't\", 'Ma', 'Mightn', \"Mightn't\", 'Mustn', \"Mustn't\", 'Needn', \"Needn't\", 'Shan', \"Shan't\", 'Shouldn', \"Shouldn't\", 'Wasn', \"Wasn't\", 'Weren', \"Weren't\", 'Won', \"Won't\", 'Wouldn', \"Wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "punctuations = list(string.punctuation)\n",
    "stop = stopwords.words('english')\n",
    "dummy_stop = []\n",
    "for word in stop:\n",
    "    dummy = word[0].upper()+word[1:]\n",
    "    dummy_stop.append(dummy)\n",
    "stop = stop+dummy_stop\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = []\n",
    "rating = []\n",
    "rating_checklist_2 = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
    "with open('all_reviews.tsv','r') as fp:\n",
    "    lines=fp.readlines()\n",
    "\n",
    "# count=0\n",
    "for line in lines:\n",
    "#     if count > 100:\n",
    "#         break\n",
    "    words=word_tokenize(line)\n",
    "    for i in range(len(words)):\n",
    "        dummy=\"\"\n",
    "        for j in range(len(words[i])):\n",
    "            if not words[i][j] in punctuations:\n",
    "                dummy += words[i][j]\n",
    "        words[i] = dummy\n",
    "    clean_words=[w for w in words if not w in stop]\n",
    "    if clean_words[len(clean_words)-1] in rating_checklist_2:\n",
    "        rating.append(clean_words[len(clean_words)-1])\n",
    "        clean_sym_words = [w for w in clean_words if w.isalpha()]\n",
    "        reviews.append(clean_sym_words[0:len(clean_sym_words)-1])\n",
    "    \n",
    "#     count += 1\n",
    "\n",
    "# print(reviews[0:5])\n",
    "# print(rating[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Google pre-trained word2vec embeddings\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('./model/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.most_similar('horrible')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #save model\n",
    "filename = \"./embedding_word2vec.txt\"\n",
    "model.save_word2vec_format(filename, binary = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = {}\n",
    "# fp = open(\"embedding_word2vec.txt\")\n",
    "# for line in fp:\n",
    "#     val = line.split()\n",
    "#     word = val[0]\n",
    "#     vector_coefs = np.asarray(val[1:])\n",
    "#     embeddings[word]=vector_coefs\n",
    "# print(embeddings)\n",
    "# fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_obj = Tokenizer()\n",
    "tokenized_obj.fit_on_texts(reviews)\n",
    "sequences = tokenized_obj.texts_to_sequences(reviews)\n",
    "# print(sequences)\n",
    "word_index = tokenized_f count > 100:\n",
    "#         breakobj.word_index\n",
    "# print(word_index)\n",
    "# print(len(word_index))\n",
    "# print(len(word_index))\n",
    "maxLength = 0\n",
    "for review in sequences:\n",
    "    if maxLength < len(review):\n",
    "        maxLength = len(review)\n",
    "# print(maxLength)\n",
    "# print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = {}\n",
    "fp = open(\"embedding_word2vec.txt\")\n",
    "for line in fp:\n",
    "    val = line.split()\n",
    "    word = val[0]\n",
    "    vector_coefs = np.asarray(val[1:])\n",
    "    if word in word_index.keys():\n",
    "        embeddings[word]=vector_coefs\n",
    "# print(embeddings)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_pad = pad_sequences(sequences, maxlen = maxLength)\n",
    "rating = np.asarray(rating)\n",
    "print(rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EmbeddingDim = 300\n",
    "num_words = len(word_index)+1\n",
    "embedding_matrix  = np.zeros((num_words, EmbeddingDim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "# print(num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(rating))\n",
    "rating_checklist = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
    "rating_matrix = np.zeros((len(rating), 11))\n",
    "for i in range(len(rating)):\n",
    "    if rating[i] in rating_checklist:\n",
    "        rating_matrix[i][int(rating[i])] = 1\n",
    "print(rating_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model = Sequential()\n",
    "embedding_layer = Embedding(num_words, EmbeddingDim, embeddings_initializer = Constant(embedding_matrix), input_length = maxLength, trainable = False)\n",
    "train_model.add(embedding_layer)\n",
    "train_model.add(GRU(units = 100))\n",
    "train_model.add(Dense(11,activation = 'sigmoid'))\n",
    "train_model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(review_pad, rating_matrix)\n",
    "# print(X_train.shape)\n",
    "# print(Y_train.shape)\n",
    "train_model.fit(X_train, Y_train, epochs = 5, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = train_model.evaluate(X_test, Y_test)\n",
    "print(accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

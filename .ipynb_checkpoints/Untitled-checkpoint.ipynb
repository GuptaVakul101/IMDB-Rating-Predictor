{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/coding-\n",
      "[nltk_data]     club/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/coding-\n",
      "[nltk_data]     club/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import gensim\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, GRU\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.initializers import Constant\n",
    "import numpy as np\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'I', 'Me', 'My', 'Myself', 'We', 'Our', 'Ours', 'Ourselves', 'You', \"You're\", \"You've\", \"You'll\", \"You'd\", 'Your', 'Yours', 'Yourself', 'Yourselves', 'He', 'Him', 'His', 'Himself', 'She', \"She's\", 'Her', 'Hers', 'Herself', 'It', \"It's\", 'Its', 'Itself', 'They', 'Them', 'Their', 'Theirs', 'Themselves', 'What', 'Which', 'Who', 'Whom', 'This', 'That', \"That'll\", 'These', 'Those', 'Am', 'Is', 'Are', 'Was', 'Were', 'Be', 'Been', 'Being', 'Have', 'Has', 'Had', 'Having', 'Do', 'Does', 'Did', 'Doing', 'A', 'An', 'The', 'And', 'But', 'If', 'Or', 'Because', 'As', 'Until', 'While', 'Of', 'At', 'By', 'For', 'With', 'About', 'Against', 'Between', 'Into', 'Through', 'During', 'Before', 'After', 'Above', 'Below', 'To', 'From', 'Up', 'Down', 'In', 'Out', 'On', 'Off', 'Over', 'Under', 'Again', 'Further', 'Then', 'Once', 'Here', 'There', 'When', 'Where', 'Why', 'How', 'All', 'Any', 'Both', 'Each', 'Few', 'More', 'Most', 'Other', 'Some', 'Such', 'No', 'Nor', 'Not', 'Only', 'Own', 'Same', 'So', 'Than', 'Too', 'Very', 'S', 'T', 'Can', 'Will', 'Just', 'Don', \"Don't\", 'Should', \"Should've\", 'Now', 'D', 'Ll', 'M', 'O', 'Re', 'Ve', 'Y', 'Ain', 'Aren', \"Aren't\", 'Couldn', \"Couldn't\", 'Didn', \"Didn't\", 'Doesn', \"Doesn't\", 'Hadn', \"Hadn't\", 'Hasn', \"Hasn't\", 'Haven', \"Haven't\", 'Isn', \"Isn't\", 'Ma', 'Mightn', \"Mightn't\", 'Mustn', \"Mustn't\", 'Needn', \"Needn't\", 'Shan', \"Shan't\", 'Shouldn', \"Shouldn't\", 'Wasn', \"Wasn't\", 'Weren', \"Weren't\", 'Won', \"Won't\", 'Wouldn', \"Wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "punctuations = list(string.punctuation)\n",
    "stop = stopwords.words('english')\n",
    "dummy_stop = []\n",
    "for word in stop:\n",
    "    dummy = word[0].upper()+word[1:]\n",
    "    dummy_stop.append(dummy)\n",
    "stop = stop+dummy_stop\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['film', 'starts', 'introducing', 'us', 'Multiculti', 'tribe', 'Switzerland', 'led', 'shaman', 'eskimo', 'woman', 'seemed', 'forgotten', 'prehistoric', 'hunter', 'gatherers', 'generally', 'wandered', 'around', 'fill', 'instead', 'days', 'waiting', 'year', 'village', 'mammoths', 'meander', 'kill', 'one', 'food', 'luckily', 'lasts', 'year', 'noble', 'existence', 'shattered', 'Arab', 'horsemen', 'looking', 'slaves', 'leave', 'Alps', 'jungles', 'Italy', 'attacked', 'birds', 'lived', 'South', 'America', 'scenery', 'changes', 'Utah', 'track', 'slavers', 'Africa', 'meet', 'Zulu', 'tribes', 'happened', 'bumped', 'Swiss', 'hunter', 'father', 'somehow', 'managed', 'teach', 'Zulu', 'tribe', 'one', 'language', 'seems', 'exist', 'Europe', 'Arab', 'desert', 'slavers', 'attacked', 'zulus', 'Swiss', 'zulus', 'combine', 'forces', 'attack', 'slavers', 'Rather', 'follow', 'river', 'Nile', 'slave', 'town', 'decide', 'cross', 'Sahara', 'food', 'water', 'river', 'would', 'seem', 'sensible', 'option', 'wandering', 'around', 'weeks', 'look', 'stars', 'decide', 'follow', 'North', 'Star', 'slave', 'city', 'common', 'Santa', 'hideaway', 'apparently', 'Hey', 'ho', 'days', 'find', 'slave', 'city', 'turns', 'pyramid', 'construction', 'site', 'led', 'alien', 'Luckily', 'crafty', 'alien', 'god', 'lots', 'slaves', 'ready', 'source', 'desert', 'living', 'woolly', 'mammoths', 'help', 'build', 'pyramid', 'Swiss', 'hunter', 'cries', 'operation', 'desert', 'freedom', 'slaves', 'rebel', 'alien', 'god', 'Indian', 'eunuchs', 'fresh', 'Indiana', 'Jones', 'Temple', 'Doom', 'albino', 'africans', 'flee', 'giant', 'ship', 'stored', 'pyramid', 'rebelling', 'slaves', 'catch', 'kill', 'giant', 'alien', 'turns', 'Lurch', 'Adams', 'family', 'Eskimo', 'woman', 'dies', 'back', 'Alps', 'bring', 'Swiss', 'hunters', 'girlfriend', 'back', 'life', 'Sahara', 'prophetic', 'shes', 'got', 'blue', 'eyes', 'apparently', 'rare', 'led', 'believe', 'Switzerland', 'film', 'ends', 'desert', 'dwelling', 'Zulus', 'giving', 'Swiss', 'crops', 'somehow', 'grew', 'Sahara', 'Swiss', 'set', 'home', 'surely', 'cursing', 'set', 'Lurch', 'giant', 'boat', 'alight', 'surely', 'would', 'speeded', 'journey', 'across', 'Mediterranean', 'group', 'hug', 'back', 'Alps', 'desert', 'crops', 'begin', 'grow', 'foot', 'glacier', 'Needless', 'say', 'wo', 'nt', 'buying'], ['know', 'go', 'cafeteria', 'style', 'restaurant', 'see', 'something', 'usually', 'enjoy', 'like', 'lasagna', 'get', 'lasagna', 'take', 'bite', 'fond', 'memories', 'last', 'time', 'ate', 'real', 'restaurant', 'first', 'taste', 'hits', 'tongue', 'hopes', 'future', 'meal', 'enjoyment', 'flushed', 'toilet', 'cafeteria', 'lasagna', 'looks', 'goods', 'potential', 'great', 'fond', 'memories', 'movies', 'genre', 'good', 'watch', 'edible', 'barely', 'movie', 'pretty', 'good', 'special', 'effects', 'nt', 'boring', 'gave', 'five', 'dialog', 'acting', 'part', 'subpar', 'story', 'nt', 'even', 'make', 'attempt', 'suspend', 'disbelief', 'Forget', 'historically', 'inaccurate', 'ridiculous', 'would', 'catch', 'matinÃ©e', 'wait', 'someone', 'else', 'pay', 'cafeteria'], ['critics', 'moaned', 'film', 'technology', 'grows', 'storytelling', 'ability', 'movies', 'shrinks', 'never', 'quite', 'agreed', 'assessment', 'believe', 'place', 'spectacle', 'variety', 'even', 'mindless', 'kind', 'However', 'share', 'view', 'critics', 'BC', 'likely', 'convincing', 'piece', 'evidence', 'argument', 'movie', 'looks', 'like', 'cost', 'millions', 'make', 'saddled', 'screenplay', 'looks', 'like', 'came', 'Dollar', 'StoreDirector', 'cowriter', 'Roland', 'Emmerich', 'stranger', 'brainless', 'spectacles', 'guy', 'brought', 'us', 'Independence', 'Day', 'Hollywood', 'take', 'Godzilla', 'fine', 'line', 'brainless', 'plain', 'brain', 'dead', 'unfortunately', 'BC', 'short', 'spectacle', 'short', 'plot', 'short', 'anything', 'people', 'go', 'movies', 'characters', 'love', 'story', 'drive', 'bare', 'bones', 'plot', 'seems', 'added', 'afterthought', 'got', 'impression', 'Emmerich', 'fellow', 'screenwriter', 'Harald', 'Kloser', 'film', 'score', 'composer', 'making', 'first', 'screenplay', 'credit', 'idea', 'couple', 'cool', 'scenes', 'tried', 'add', 'bunch', 'filler', 'material', 'threw', 'sketchy', 'characters', 'hardly', 'reach', 'two', 'dimensions', 'inhabit', 'filler', 'called', 'screenplay', 'order', 'spectacle', 'work', 'even', 'cheesefilled', 'variety', 'something', 'audience', 'get', 'excited', 'movie', 'one', 'big', 'teaseThe', 'plot', 'even', 'called', 'set', 'days', 'early', 'man', 'heroes', 'unnamed', 'tribal', 'people', 'speak', 'perfect', 'English', 'bodies', 'supermodels', 'hunt', 'mammoths', 'food', 'two', 'characters', 'supposed', 'focused', 'pair', 'young', 'lovers', 'named', 'DLeh', 'Steven', 'Strait', 'Evolet', 'Camilla', 'Belle', 'love', 'care', 'movie', 'never', 'goes', 'way', 'explain', 'rest', 'villagers', 'really', 'matter', 'exist', 'simply', 'captured', 'group', 'foreign', 'invaders', 'come', 'riding', 'peaceful', 'tribe', 'kidnap', 'work', 'slaves', 'back', 'home', 'colony', 'Evolet', 'one', 'captured', 'DLeh', 'small', 'handful', 'others', 'set', 'find', 'taken', 'seek', 'aid', 'tribes', 'also', 'invaded', 'enemy', 'mammoth', 'herd', 'saber', 'tooth', 'tiger', 'nothing', 'anything', 'computer', 'generated', 'special', 'effects', 'simply', 'filmmakers', 'felt', 'current', 'scene', 'needed', 'special', 'effect', 'shot', 'impressed', 'effects', 'nt', 'look', 'place', 'actors', 'BC', 'probably', 'would', 'worked', 'better', 'silent', 'movie', 'subtitled', 'one', 'dialogue', 'comes', 'mouths', 'people', 'wooden', 'spears', 'carry', 'good', 'tribes', 'people', 'movie', 'mastered', 'Queen', 'English', 'naturally', 'evil', 'invading', 'tribe', 'speak', 'subtitles', 'sometimes', 'voices', 'mechanically', 'altered', 'lowered', 'sound', 'threatening', 'demonic', 'one', 'movie', 'allowed', 'personality', 'act', 'differently', 'one', 'another', 'Everybody', 'tribe', 'talks', 'thinks', 'behaves', 'exactly', 'facial', 'hair', 'differing', 'body', 'types', 'main', 'way', 'tell', 'apart', 'would', 'make', 'hard', 'get', 'involved', 'story', 'movie', 'dodges', 'tricky', 'issue', 'even', 'story', 'first', 'place', 'film', 'main', 'tribe', 'attacked', 'movie', 'turns', 'endless', 'string', 'filler', 'material', 'padding', 'drag', 'whole', 'thing', 'feature', 'length', 'Aside', 'brief', 'encounter', 'birdlike', 'prehistoric', 'creatures', 'moments', 'action', 'danger', 'DLeh', 'followers', 'reach', 'land', 'invading', 'army', 'movie', 'throws', 'saber', 'tooth', 'tiger', 'encounter', 'fool', 'us', 'thinking', 'something', 'gon', 'na', 'happen', 'tiger', 'winds', 'boring', 'human', 'characters', 'inhabiting', 'movie', 'millions', 'special', 'effects', 'budget', 'wasted', 'something', 'nt', 'need', 'first', 'place', 'move', 'shaky', 'plot', 'alongThere', 'key', 'ingredient', 'missing', 'BC', 'fun', 'movie', 'fun', 'watch', 'kept', 'waiting', 'something', 'anything', 'happen', 'something', 'eventually', 'happen', 'usually', 'underwhelming', 'know', 'people', 'interested', 'seeing', 'movie', 'special', 'effects', 'think', 'looks', 'enjoyably', 'cheesy', 'people', 'say', 'please', 'drawn', 'curiosity', 'nt', 'even', 'enjoyable', 'bad', 'sense', 'precious', 'time', 'worth', 'theater', 'may', 'charging', 'see', 'movie', 'anyone', 'wondering', 'yes', 'includes', 'budget', 'cinema', 'price'], ['must', 'easily', 'impressed', 'convinced', 'maybe', 'love', 'movies', 'much', 'although', 'plenty', 'hate', 'movie', 'getting', 'skewered', 'professional', 'critics', 'IMDb', 'critics', 'alike', 'really', 'make', 'second', 'guess', 'judgment', 'never', 'let', 'crowd', 'tell', 'way', 'go', 'going', 'start', 'meager', 'expectations', 'BC', 'really', 'sure', 'expect', 'also', 'HUGE', 'History', 'buff', 'studied', 'Sumerian', 'religion', 'culture', 'lot', 'High', 'School', 'yes', 'Sumerian', 'NOT', 'Egyptians', 'like', 'everyone', 'calling', 'anything', 'else', 'BC', 'stunning', 'jaw', 'dropping', 'spectacle', 'hunting', 'Mammoths', 'incredible', 'pyramid', 'building', 'battles', 'ONE', 'thing', 'wrong', 'film', 'rating', 'family', 'film', 'nasty', 'dude', 'wants', 'blood', 'guts', 'gore', 'everything', 'see', 'film', 'needed', 'adult', 'content', 'needed', 'see', 'battle', 'ruggedness', 'lives', 'violence', 'much', 'like', 'Apocalypto', 'understand', 'wanting', 'keep', 'options', 'open', 'make', 'money', 'believe', 'BC', 'made', 'historical', 'grand', 'epic', 'would', 'take', 'home', 'awards', 'make', 'money', 'period', 'believe', 'also', 'common', 'place', 'plot', 'holes', 'weird', 'little', 'things', 'could', 'picked', 'apart', 'sit', 'back', 'watch', 'spectacle', 'enjoy', 'think', 'masterpiece', 'rightThe', 'cast', 'adequate', 'nt', 'think', 'anyone', 'person', 'really', 'stuns', 'amazes', 'good', 'respective', 'roles', 'Relative', 'newcomer', 'Steven', 'Strait', 'plays', 'lead', 'hero', 'man', 'grew', 'believing', 'father', 'abandoned', 'people', 'discovers', 'destined', 'lead', 'good', 'watchable', 'nt', 'light', 'screen', 'anything', 'nt', 'really', 'command', 'huge', 'presence', 'screen', 'Camilla', 'Belle', 'plays', 'love', 'interest', 'stunning', 'really', 'beautiful', 'especially', 'eyes', 'fake', 'really', 'nothing', 'eye', 'candy', 'damsel', 'distress', 'think', 'could', 'better', 'Character', 'actor', 'Cliff', 'Curtis', 'elder', 'longs', 'teach', 'Strait', 'character', 'befriends', 'character', 'could', 'lot', 'important', 'forefront', 'kind', 'quiet', 'foreboding', 'loses', 'lot', 'goes', 'Affif', 'Ben', 'Badra', 'plays', 'villain', 'taken', 'lovely', 'Belle', 'almost', 'expected', 'sort', 'redemption', 'character', 'never', 'happens', 'quite', 'opposite', 'fact', 'Oscar', 'nominated', 'veteran', 'actor', 'Omar', 'Sharif', 'narrator', 'nt', 'use', 'much', 'unfortunateIt', 'seems', 'like', 'comes', 'cast', 'underused', 'lot', 'potential', 'exchanging', 'visual', 'effects', 'works', 'CGI', 'incredible', 'sabertooth', 'tiger', 'stunning', 'Mammoths', 'pyramids', 'main', 'villain', 'god', 'Pyramids', 'disturbing', 'ever', 'historical', 'inaccuracy', 'really', 'nt', 'think', 'film', 'trying', 'BUT', 'inaccurate', 'everyone', 'crying', 'villains', 'film', 'NOT', 'Egyptians', 'Sumerians', 'lot', 'history', 'mostly', 'legend', 'lore', 'think', 'incredible', 'job', 'covering', 'even', 'make', 'mention', 'possibility', 'Atlantis', 'give', 'whole', 'actual', 'visual', 'putting', 'together', 'pyramids', 'original', 'face', 'Sphinx', 'looked', 'awful', 'cartoony', 'still', 'film', 'PG', 'get', 'never', 'showing', 'skin', 'using', 'foul', 'language', 'brutal', 'violence', 'battle', 'scenes', 'still', 'cool', 'never', 'see', 'close', 'anyone', 'dying', 'bring', 'tweens', 'see', 'good', 'different', 'spin', 'historical', 'epic', 'everyone', 'Director', 'Roland', 'Emmerich', 'done', 'really', 'incredible', 'stuff', 'Independence', 'Day', 'disappointing', 'average', 'stuff', 'Godzilla', 'real', 'desire', 'monstrous', 'things', 'budget', 'sometimes', 'work', 'sometimes', 'wo', 'nt', 'nt', 'chastise', 'wanting', 'entertain', 'us', 'good', 'director', 'see', 'style', 'desire', 'entertain', 'say', 'fight', 'harsh', 'critics', 'beginning', 'summer', 'blockbuster', 'pretty', 'incredible'], ['best', 'view', 'movie', 'proper', 'expectations', 'certainly', 'nt', 'designed', 'realistic', 'historically', 'accurate', 'portrayal', 'times', 'better', 'serves', 'mythological', 'tale', 'human', 'struggle', 'experienced', 'fictional', 'tribe', 'somewhere', 'North', 'Himalayan', 'mountains', 'able', 'learn', 'interaction', 'leader', 'Dhel', 'journey', 'tribes', 'recapture', 'people', 'taken', 'slaves', 'advanced', 'civilizationYes', 'many', 'inconsistencies', 'film', 'relates', 'time', 'place', 'languages', 'spoken', 'Even', 'amusing', 'existence', 'jungle', 'roaming', 'carnivorous', 'ostriches', 'never', 'existed', 'along', 'sabre', 'tooth', 'tigers', 'wooly', 'mammoths', 'long', 'extinct', 'appreciated', 'movie', 'struggle', 'mankind', 'including', 'personal', 'insecurities', 'overcome', 'cooperation', 'developed', 'vested', 'interest', 'unite', 'vanquish', 'common', 'enemy', 'respect', 'movie', 'compared', 'challenges', 'faced', 'throughout', 'history', 'continue', 'daySome', 'embellishments', 'include', 'protagonist', 'modest', 'crew', 'crossing', 'Himalayas', 'keeping', 'pace', 'demons', 'four', 'legs', 'Egyptians', 'horseback', 'captured', 'villagers', 'including', 'cherished', 'Evolet', 'extreme', 'distance', 'journey', 'far', 'exceeds', 'possible', 'range', 'covered', 'peoples', 'though', 'nomadic', 'usually', 'never', 'wandered', 'hundred', 'square', 'miles', 'origins', 'Despite', 'harsh', 'realities', 'witness', 'grim', 'meanderings', 'across', 'Himalayas', 'Indian', 'jungles', 'across', 'Middle', 'East', 'lastly', 'join', 'forces', 'African', 'tribes', 'along', 'Nile', 'even', 'dragging', 'injured', 'journey', 'magnitude', 'would', 'possible', 'another', 'years', 'Mesopotamians', 'domesticated', 'horses', 'first', 'placeHowever', 'considering', 'movie', 'context', 'rather', 'content', 'BC', 'becomes', 'intriguing', 'diversion', 'realistic', 'entertainment', 'alternative', 'reality']]\n",
      "['1', '5', '1', '9', '7']\n"
     ]
    }
   ],
   "source": [
    "reviews = []\n",
    "rating = []\n",
    "rating_checklist_2 = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
    "with open('all_reviews.tsv','r') as fp:\n",
    "    lines=fp.readlines()\n",
    "\n",
    "# count=0\n",
    "for line in lines:\n",
    "#     if count > 100:\n",
    "#         break\n",
    "    words=word_tokenize(line)\n",
    "    for i in range(len(words)):\n",
    "        dummy=\"\"\n",
    "        for j in range(len(words[i])):\n",
    "            if not words[i][j] in punctuations:\n",
    "                dummy += words[i][j]\n",
    "        words[i] = dummy\n",
    "    clean_words=[w for w in words if not w in stop]\n",
    "    if clean_words[len(clean_words)-1] in rating_checklist_2:\n",
    "        rating.append(clean_words[len(clean_words)-1])\n",
    "        clean_sym_words = [w for w in clean_words if w.isalpha()]\n",
    "        reviews.append(clean_sym_words[0:len(clean_sym_words)-1])\n",
    "    \n",
    "#     count += 1\n",
    "\n",
    "# print(reviews[0:5])\n",
    "# print(rating[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-6893af40bf85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Using Google pre-trained word2vec embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./model/GoogleNews-vectors-negative300.bin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1474\u001b[0m         return _load_word2vec_format(\n\u001b[1;32m   1475\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1476\u001b[0;31m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[1;32m   1477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1478\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    381\u001b[0m                         \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m                 \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m                 \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_deprecation_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m                     \u001b[0;31m# TODO use frombuffer or something similar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m                     \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfromstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinary_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mREAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36mhelper\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhelper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_GeneratorContextManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhelper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Using Google pre-trained word2vec embeddings\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('./model/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.most_similar('horrible')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #save model\n",
    "filename = \"./embedding_word2vec.txt\"\n",
    "model.save_word2vec_format(filename, binary = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = {}\n",
    "# fp = open(\"embedding_word2vec.txt\")\n",
    "# for line in fp:\n",
    "#     val = line.split()\n",
    "#     word = val[0]\n",
    "#     vector_coefs = np.asarray(val[1:])\n",
    "#     embeddings[word]=vector_coefs\n",
    "# print(embeddings)\n",
    "# fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_obj = Tokenizer()\n",
    "tokenized_obj.fit_on_texts(reviews)\n",
    "sequences = tokenized_obj.texts_to_sequences(reviews)\n",
    "# print(sequences)\n",
    "word_index = tokenized_obj.word_index\n",
    "# print(word_index)\n",
    "# print(len(word_index))\n",
    "# print(len(word_index))\n",
    "maxLength = 0\n",
    "for review in sequences:\n",
    "    if maxLength < len(review):\n",
    "        maxLength = len(review)\n",
    "# print(maxLength)\n",
    "# print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = {}\n",
    "fp = open(\"embedding_word2vec.txt\")\n",
    "for line in fp:\n",
    "    val = line.split()\n",
    "    word = val[0]\n",
    "    vector_coefs = np.asarray(val[1:])\n",
    "    if word in word_index.keys():\n",
    "        embeddings[word]=vector_coefs\n",
    "# print(embeddings)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1' '5' '1' '9' '7' '1' '1' '7' '4' '10' '3' '1' '10' '9' '8' '1' '3'\n",
      " '10' '1' '1' '1' '10' '10' '3' '3' '2' '2' '5' '4' '3' '1' '1' '2' '1'\n",
      " '10' '5' '10' '9' '1' '5' '2' '4' '1' '1' '1' '3' '4' '2' '1' '8' '5' '6'\n",
      " '1' '9' '5' '7' '1' '10' '7' '4' '1' '1' '1' '1' '8' '8' '1' '3' '1' '10'\n",
      " '2' '5' '1' '4' '8' '3' '8' '7' '7' '6' '1' '1' '1' '3' '1' '7' '2' '1'\n",
      " '1' '8' '1' '5' '3' '9' '7' '2' '4' '9' '10' '1' '9']\n"
     ]
    }
   ],
   "source": [
    "review_pad = pad_sequences(sequences, maxlen = maxLength)\n",
    "rating = np.asarray(rating)\n",
    "print(rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-102b284761fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0membedding_matrix\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEmbeddingDim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0membedding_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0membedding_vector\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0membedding_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "EmbeddingDim = 300\n",
    "num_words = len(word_index)+1\n",
    "embedding_matrix  = np.zeros((num_words, EmbeddingDim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "# print(num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# print(len(rating))\n",
    "rating_checklist = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
    "rating_matrix = np.zeros((len(rating), 11))\n",
    "for i in range(len(rating)):\n",
    "    if rating[i] in rating_checklist:\n",
    "        rating_matrix[i][int(rating[i])] = 1\n",
    "print(rating_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model = Sequential()\n",
    "embedding_layer = Embedding(num_words, EmbeddingDim, embeddings_initializer = Constant(embedding_matrix), input_length = maxLength, trainable = False)\n",
    "train_model.add(embedding_layer)\n",
    "train_model.add(GRU(units = 100))\n",
    "train_model.add(Dense(11,activation = 'sigmoid'))\n",
    "train_model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(review_pad, rating_matrix)\n",
    "# print(X_train.shape)\n",
    "# print(Y_train.shape)\n",
    "train_model.fit(X_train, Y_train, epochs = 5, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = train_model.evaluate(X_test, Y_test)\n",
    "print(accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
